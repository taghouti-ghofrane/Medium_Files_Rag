import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from lightrag import LightRAG
from lightrag.utils import EmbeddingFunc
from raganything import RAGAnything
import os
import logging
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# =======================
# å·¥å…·å‡½æ•°ï¼šè°ƒç”¨ Ollama APIï¼ˆå¼‚æ­¥ï¼‰
# =======================

async def ollama_complete_async(
    model: str,
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: List[Dict[str, str]] = [],
    image_data: Optional[str] = None,
    **kwargs
) -> str:
    """
    å¼‚æ­¥è°ƒç”¨ Ollamaï¼Œæ”¯æŒæ–‡æœ¬ + å›¾åƒè¾“å…¥
    """
    messages = []

    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    for msg in history_messages:
        messages.append({"role": msg["role"], "content": msg["content"]})

    # æž„é€  contentï¼šæ”¯æŒæ–‡æœ¬ + å›¾åƒ
    content = [{"type": "text", "text": prompt}]
    if image_data:
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
        })

    messages.append({"role": "user", "content": content})

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
    }

    async with aiohttp.ClientSession() as session:
        async with session.post("http://localhost:11434/v1/chat/completions", json=payload) as resp:
            if resp.status != 200:
                text = await resp.text()
                raise Exception(f"Ollama error {resp.status}: {text}")
            result = await resp.json()
            return result["choices"][0]["message"]["content"]


async def ollama_embed_async(texts: List[str], model: str = "bge-m3:latest") -> List[List[float]]:
    """
    å¼‚æ­¥èŽ·å–åµŒå…¥å‘é‡
    """
    embeddings = []
    async with aiohttp.ClientSession() as session:
        for text in texts:
            if not text.strip():
                embeddings.append([0.0] * 1024)
                continue
            payload = {"model": model, "input": text}
            try:
                async with session.post("http://localhost:11434/api/embeddings", json=payload) as resp:
                    if resp.status != 200:
                        text_resp = await resp.text()
                        logger.warning(f"Embedding error for text '{text[:50]}...': {text_resp}")
                        embeddings.append([0.0] * 1024)
                    else:
                        result = await resp.json()
                        embedding = result["embedding"]
                        # è¡¥é½æˆ–æˆªæ–­åˆ° 1024 ç»´åº¦
                        if len(embedding) > 1024:
                            embedding = embedding[:1024]
                        elif len(embedding) < 1024:
                            embedding += [0.0] * (1024 - len(embedding))
                        embeddings.append(embedding)
            except Exception as e:
                logger.warning(f"Embedding exception for text '{text[:50]}...': {e}")
                embeddings.append([0.0] * 1024)
    return embeddings


# =======================
# åŒ…è£…å‡½æ•°ï¼ˆåœ¨å¼‚æ­¥çŽ¯å¢ƒä¸­ç›´æŽ¥è°ƒç”¨ï¼‰
# =======================

async def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
    return await ollama_complete_async(
        model=LLM_MODEL,
        prompt=prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        **kwargs
    )

async def embedding_func(texts):
    return await ollama_embed_async(texts, EMBED_MODEL)

async def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs):
    return await ollama_complete_async(
        model=VLM_MODEL,
        prompt=prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        image_data=image_data,
        **kwargs
    )


# =======================
# LightRAG é…ç½®
# =======================

LLM_MODEL = "qwen3:8b"
EMBED_MODEL = "bge-m3:latest"
VLM_MODEL = "llava:latest"

lightrag_working_dir = "./rag_storage_new"

# åˆ›å»ºæˆ–åŠ è½½ LightRAG å®žä¾‹
lightrag_instance = LightRAG(
    working_dir=lightrag_working_dir,
    llm_model_func=llm_model_func,
    embedding_func=EmbeddingFunc(
        embedding_dim=1024,
        max_token_size=5122,
        func=embedding_func
    )
)

# =======================
# åˆå§‹åŒ– RAGAnything
# =======================

rag = RAGAnything(
    lightrag=lightrag_instance,
    vision_model_func=vision_model_func
)


# =======================
# ç¤ºä¾‹æŸ¥è¯¢
# =======================

async def main():
    print("âœ… æ­£åœ¨åŠ è½½å·²å­˜åœ¨çš„ LightRAG å®žä¾‹...")
    await lightrag_instance.initialize_storages()

    # æŸ¥è¯¢ç¤ºä¾‹
    result = await rag.aquery("è¿™ä¸ªçŸ¥è¯†åº“ä¸­å¤„ç†äº†å“ªäº›æ•°æ®ï¼Ÿ", mode="hybrid")
    print("ðŸ” æŸ¥è¯¢ç»“æžœ:", result)

    # å¯é€‰ï¼šæ·»åŠ æ–°æ–‡æ¡£
    # await rag.process_document_complete(file_path="path/to/new/multimodal_document.pdf", output_dir="./output")


if __name__ == "__main__":
    asyncio.run(main())