import asyncio
import aiohttp
import base64
from typing import List, Dict, Any, Optional
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.utils import EmbeddingFunc
import os
import shutil
import logging
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¦‚æœè¦ä½¿ç”¨ rerankï¼Œéœ€è¦å®‰è£… sentence-transformers
try:
    from sentence_transformers import CrossEncoder
    RERANK_AVAILABLE = True
except ImportError:
    RERANK_AVAILABLE = False


# =======================
# å·¥å…·å‡½æ•°ï¼šè°ƒç”¨ Ollama APIï¼ˆå¼‚æ­¥ï¼‰
# =======================

async def ollama_complete_async(
    model: str,
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: List[Dict[str, str]] = [],
    image_data: Optional[str] = None,
    **kwargs
) -> str:
    """
    å¼‚æ­¥è°ƒç”¨ Ollamaï¼Œæ”¯æŒæ–‡æœ¬ + å›¾åƒè¾“å…¥
    """
    messages = []

    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    for msg in history_messages:
        messages.append({"role": msg["role"], "content": msg["content"]})

    # æ„é€  contentï¼šæ”¯æŒæ–‡æœ¬ + å›¾åƒ
    content = [{"type": "text", "text": prompt}]
    if image_data:
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
        })

    messages.append({"role": "user", "content": content})

    # æ„å»º payload
    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
    }

    # è¿‡æ»¤ kwargsï¼Œåªä¿ç•™å¯ JSON åºåˆ—åŒ–çš„åŸºç¡€ç±»å‹
    safe_kwargs = {
        k: v for k, v in kwargs.items()
        if isinstance(v, (str, int, float, bool, type(None)))
    }
    payload.update(safe_kwargs)

    async with aiohttp.ClientSession() as session:
        async with session.post("http://localhost:11434/v1/chat/completions", json=payload) as resp:
            if resp.status != 200:
                text = await resp.text()
                raise Exception(f"Ollama error {resp.status}: {text}")
            result = await resp.json()
            return result["choices"][0]["message"]["content"]


async def ollama_embed_async(texts: List[str], model: str = "bge-m3:latest") -> List[List[float]]:
    """
    å¼‚æ­¥è·å–åµŒå…¥å‘é‡ï¼Œç¡®ä¿è¿”å›æ­£ç¡®çš„ç»´åº¦
    """
    embeddings = []
    async with aiohttp.ClientSession() as session:
        for text in texts:
            if not text.strip():  # å¤„ç†ç©ºæ–‡æœ¬
                # è¿”å›é›¶å‘é‡
                embeddings.append([0.0] * 1024)
                continue
                
            payload = {"model": model, "input": text}
            try:
                async with session.post("http://localhost:11434/api/embeddings", json=payload) as resp:
                    if resp.status != 200:
                        text_resp = await resp.text()
                        logger.warning(f"Embedding error for text '{text[:50]}...': {text_resp}")
                        # è¿”å›é›¶å‘é‡ä½œä¸ºåå¤‡
                        embeddings.append([0.0] * 1024)
                    else:
                        result = await resp.json()
                        embedding = result["embedding"]
                        # ç¡®ä¿ç»´åº¦æ­£ç¡®
                        if len(embedding) != 1024:
                            logger.warning(f"Embedding dimension mismatch: expected 1024, got {len(embedding)}")
                            # è°ƒæ•´ç»´åº¦
                            if len(embedding) > 1024:
                                embedding = embedding[:1024]
                            else:
                                embedding.extend([0.0] * (1024 - len(embedding)))
                        embeddings.append(embedding)
            except Exception as e:
                logger.warning(f"Embedding exception for text '{text[:50]}...': {e}")
                # è¿”å›é›¶å‘é‡ä½œä¸ºåå¤‡
                embeddings.append([0.0] * 1024)
    return embeddings


# =======================
# å¼‚æ­¥åµŒå…¥å‡½æ•°åŒ…è£…å™¨
# =======================

class AsyncEmbeddingWrapper:
    def __init__(self, model: str = "bge-m3:latest"):
        self.model = model
    
    async def embed(self, texts: List[str]) -> List[List[float]]:
        return await ollama_embed_async(texts, self.model)


# =======================
# Rerank å‡½æ•°ï¼ˆå¯é€‰ï¼‰
# =======================

rerank_model_func = None
if RERANK_AVAILABLE:
    try:
        _reranker = CrossEncoder("BAAI/bge-reranker-base")
        
        def rerank_model_func(query: str, docs: List[str]) -> List[str]:
            if not docs:
                return []
            pairs = [(query, doc) for doc in docs]
            scores = _reranker.predict(pairs)
            ranked = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
            return [doc for doc, _ in ranked]
    except Exception as e:
        logger.warning(f"Rerank model initialization failed: {e}")
        rerank_model_func = None


# =======================
# ä¸»å‡½æ•°
# =======================
async def main():
    # è®¾ç½®æ¨¡å‹åç§°ï¼ˆç¡®ä¿å·²ç”¨ ollama pull ä¸‹è½½ï¼‰
    LLM_MODEL = "qwen3:8b"           # æ–‡æœ¬æ¨¡å‹
    EMBED_MODEL = "bge-m3:latest"    # åµŒå…¥æ¨¡å‹
    VLM_MODEL = "llava:latest"       # è§†è§‰è¯­è¨€æ¨¡å‹

    # åˆ›å»ºåµŒå…¥åŒ…è£…å™¨å®ä¾‹
    embedding_wrapper = AsyncEmbeddingWrapper(EMBED_MODEL)

    # åˆ›å»º RAGAnything é…ç½®
    config = RAGAnythingConfig(
        working_dir="./rag_storage_new",
        parser="mineru",  # é€‰æ‹©è§£æå™¨ï¼šmineru æˆ– docling
        parse_method="auto",
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # LLM å‡½æ•°ï¼ˆå¼‚æ­¥ï¼‰
    async def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return await ollama_complete_async(
            model=LLM_MODEL,
            prompt=prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            **kwargs
        )

    # è§†è§‰æ¨¡å‹å‡½æ•°ï¼ˆæ”¯æŒå›¾åƒè¾“å…¥ï¼‰
    async def vision_model_func(
        prompt,
        system_prompt=None,
        history_messages=[],
        image_data=None,
        **kwargs
    ):
        return await ollama_complete_async(
            model=VLM_MODEL,
            prompt=prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            image_data=image_data,
            **kwargs
        )

    # åµŒå…¥å‡½æ•°ï¼ˆçœŸæ­£çš„å¼‚æ­¥å‡½æ•°ï¼‰
    async def async_embedding_func(texts: List[str]) -> List[List[float]]:
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        filtered_texts = [text if text.strip() else " " for text in texts]
        return await embedding_wrapper.embed(filtered_texts)

    # åµŒå…¥å‡½æ•°åŒ…è£…æˆ EmbeddingFunc æ ¼å¼
    embedding_func = EmbeddingFunc(
        embedding_dim=1024,  # bge-m3 æ˜¯ 1024 ç»´
        max_token_size=512, # æ”¯æŒé•¿æ–‡æœ¬
        func=async_embedding_func,
    )

    # åˆå§‹åŒ– RAGAnythingï¼ˆä¸ä½¿ç”¨ rerank_model_func å‚æ•°ï¼‰
    rag_args = {
        "config": config,
        "llm_model_func": llm_model_func,
        "vision_model_func": vision_model_func,
        "embedding_func": embedding_func,
    }

    # åªæœ‰åœ¨æ”¯æŒçš„æƒ…å†µä¸‹æ‰æ·»åŠ  rerank_model_func
    if rerank_model_func is not None:
        try:
            rag = RAGAnything(**rag_args, rerank_model_func=rerank_model_func)
        except TypeError:
            logger.warning("Current RAGAnything version doesn't support rerank_model_func, initializing without it")
            rag = RAGAnything(**rag_args)
    else:
        rag = RAGAnything(**rag_args)

    # Process Documentså‰å…ˆæ¸…ç†æ—§å­˜å‚¨ï¼ˆæ‰‹åŠ¨æ–¹å¼ï¼‰
    if os.path.exists("./rag_storage_new"):
        try:
            shutil.rmtree("./rag_storage_new")
            print("ğŸ§¹ å·²æ¸…ç†æ—§çš„å­˜å‚¨ç›®å½•")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†æ—§ç›®å½•å¤±è´¥: {e}")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs("./output", exist_ok=True)

    # Process Documents
    try:
        print("ğŸš€ å¼€å§‹Process Documents...")
        await rag.process_document_complete(
            file_path=r"D:\adavance\tsy\rag4chat\test.pdf",  # ä¿®æ”¹ä¸ºä½ çš„ PDF è·¯å¾„
            output_dir="./output",
            parse_method="auto"
        )
        print("âœ… æ–‡æ¡£å¤„ç†å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return

    try:
        print("ğŸ” æ‰§è¡Œæ–‡æœ¬æŸ¥è¯¢...")
        # æ–‡æœ¬æŸ¥è¯¢
        text_result = await rag.aquery(
            "æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            mode="hybrid"
        )
        print("ğŸ“„ æ–‡æœ¬æŸ¥è¯¢ç»“æœ:", text_result)
    except Exception as e:
        print(f"âŒ æ–‡æœ¬æŸ¥è¯¢å¤±è´¥: {e}")
        # ä¸è¦è®©æŸ¥è¯¢å¤±è´¥å½±å“ç¨‹åºç»§ç»­è¿è¡Œ
        pass

    try:
        print("ğŸ“Š æ‰§è¡Œå¤šæ¨¡æ€æŸ¥è¯¢...")
        # å¤šæ¨¡æ€æŸ¥è¯¢ï¼ˆè¡¨æ ¼ï¼‰- ä¿®å¤æ ¼å¼
        table_md = """| ç³»ç»Ÿ | å‡†ç¡®ç‡ | F1åˆ†æ•° |
|------|--------|-------|
| RAGAnything | 95.2% | 0.94 |
| åŸºå‡†æ–¹æ³• | 87.3% | 0.85 |"""

        # ä¿®å¤å¤šæ¨¡æ€å†…å®¹æ ¼å¼
        multimodal_result = await rag.aquery_with_multimodal(
            "åˆ†æè¿™ä¸ªæ€§èƒ½æ•°æ®å¹¶è§£é‡Šä¸ç°æœ‰æ–‡æ¡£å†…å®¹çš„å…³ç³»",
            multimodal_content=[
                {
                    "type": "table",
                    "content": table_md,
                    "description": "æ€§èƒ½å¯¹æ¯”ç»“æœ"
                }
            ],
            mode="hybrid"
        )
        print("ğŸ“Š å¤šæ¨¡æ€æŸ¥è¯¢ç»“æœ:", multimodal_result)
    except Exception as e:
        print(f"âŒ å¤šæ¨¡æ€æŸ¥è¯¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


# æ·»åŠ ä¸€ä¸ªç®€å•çš„æµ‹è¯•å‡½æ•°
async def test_embedding():
    """æµ‹è¯•åµŒå…¥åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åµŒå…¥åŠŸèƒ½...")
    try:
        test_texts = ["Hello world", "This is a test", ""]
        embeddings = await ollama_embed_async(test_texts, "bge-m3:latest")
        print(f"âœ… åµŒå…¥æµ‹è¯•æˆåŠŸï¼Œè¿”å› {len(embeddings)} ä¸ªå‘é‡")
        for i, emb in enumerate(embeddings):
            print(f"  æ–‡æœ¬ {i}: ç»´åº¦ {len(emb)}")
    except Exception as e:
        print(f"âŒ åµŒå…¥æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # å¯ä»¥å…ˆè¿è¡Œæµ‹è¯•
    # asyncio.run(test_embedding())
    
    # è¿è¡Œä¸»ç¨‹åº
    asyncio.run(main())