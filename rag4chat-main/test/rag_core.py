"""RAG æ ¸å¿ƒç±» - ä½¿ç”¨å·²å­˜åœ¨çš„ LightRAG å®ä¾‹"""
import asyncio
import aiohttp
import os
import json
from typing import List, Dict, Any, Optional
from raganything import RAGAnything
from lightrag import LightRAG
from lightrag.utils import EmbeddingFunc
from config import LLM_MODEL, EMBED_MODEL, VLM_MODEL, WORKING_DIR, OUTPUT_DIR, DB_INFO_FILE

async def ollama_complete_async(
    model: str,
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: List[Dict[str, str]] = [],
    image_data: Optional[str] = None,
    **kwargs
) -> str:
    """å¼‚æ­¥è°ƒç”¨ Ollama"""
    messages = []

    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    for msg in history_messages:
        messages.append({"role": msg["role"], "content": msg["content"]})

    content = [{"type": "text", "text": prompt}]
    if image_data:
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
        })

    messages.append({"role": "user", "content": content})

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
    }

    safe_kwargs = {
        k: v for k, v in kwargs.items()
        if isinstance(v, (str, int, float, bool, type(None)))
    }
    payload.update(safe_kwargs)

    async with aiohttp.ClientSession() as session:
        async with session.post("http://localhost:11434/v1/chat/completions", json=payload) as resp:
            if resp.status != 200:
                text = await resp.text()
                raise Exception(f"Ollama error {resp.status}: {text}")
            result = await resp.json()
            return result["choices"][0]["message"]["content"]

async def ollama_embed_async(texts: List[str], model: str = "bge-m3:latest") -> List[List[float]]:
    """å¼‚æ­¥è·å–åµŒå…¥å‘é‡"""
    embeddings = []
    async with aiohttp.ClientSession() as session:
        for text in texts:
            if not text.strip():
                embeddings.append([0.0] * 1024)
                continue
                
            payload = {"model": model, "input": text}
            try:
                async with session.post("http://localhost:11434/api/embeddings", json=payload) as resp:
                    if resp.status != 200:
                        text_resp = await resp.text()
                        print(f"âš ï¸ Embedding warning for text '{text[:50]}...': {text_resp}")
                        embeddings.append([0.0] * 1024)
                    else:
                        result = await resp.json()
                        embedding = result["embedding"]
                        if len(embedding) != 1024:
                            print(f"âš ï¸ Embedding dimension mismatch: expected 1024, got {len(embedding)}")
                            if len(embedding) > 1024:
                                embedding = embedding[:1024]
                            else:
                                embedding.extend([0.0] * (1024 - len(embedding)))
                        embeddings.append(embedding)
            except Exception as e:
                print(f"âš ï¸ Embedding exception for text '{text[:50]}...': {e}")
                embeddings.append([0.0] * 1024)
    return embeddings

class RAGCore:
    def __init__(self):
        self.rag = None
        self.lightrag_instance = None
        
    async def initialize_with_existing_lightrag(self):
        """ä½¿ç”¨å·²å­˜åœ¨çš„ LightRAG å®ä¾‹åˆå§‹åŒ–"""
        print("ğŸ”„ åˆå§‹åŒ– RAG ç³»ç»Ÿï¼ˆä½¿ç”¨å·²å­˜åœ¨çš„ LightRAG å®ä¾‹ï¼‰...")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²æ„å»ºçš„æ•°æ®åº“
        if not os.path.exists(WORKING_DIR):
            os.makedirs(WORKING_DIR, exist_ok=True)
            
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ LightRAG æ•°æ®
        has_existing_data = (
            os.path.exists(WORKING_DIR) and 
            os.listdir(WORKING_DIR) and
            any(f for f in os.listdir(WORKING_DIR) if f.endswith('.json') or f.endswith('.graphml'))
        )
        
        if has_existing_data:
            print("âœ… å‘ç°å·²å­˜åœ¨çš„ LightRAG å®ä¾‹ï¼Œæ­£åœ¨åŠ è½½...")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°å·²å­˜åœ¨çš„ LightRAG å®ä¾‹ï¼Œå°†åˆ›å»ºæ–°å®ä¾‹")
        
        # åˆ›å»º LightRAG å®ä¾‹
        def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨åŒæ­¥ç‰ˆæœ¬ï¼Œå› ä¸º LightRAG å¯èƒ½æœŸæœ›åŒæ­¥å‡½æ•°
            return asyncio.run(ollama_complete_async(
                model=LLM_MODEL,
                prompt=prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                **kwargs
            ))

        def embedding_func(texts: List[str]) -> List[List[float]]:
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨åŒæ­¥ç‰ˆæœ¬
            return asyncio.run(ollama_embed_async(texts, EMBED_MODEL))

        # åˆ›å»º LightRAG å®ä¾‹
        self.lightrag_instance = LightRAG(
            working_dir=WORKING_DIR,
            llm_model_func=llm_model_func,
            embedding_func=EmbeddingFunc(
                embedding_dim=1024,
                max_token_size=8192,
                func=embedding_func,
            )
        )
        
        # å®šä¹‰è§†è§‰æ¨¡å‹å‡½æ•°
        async def vision_model_func(
            prompt,
            system_prompt=None,
            history_messages=[],
            image_data=None,
            **kwargs
        ):
            return await ollama_complete_async(
                model=VLM_MODEL,
                prompt=prompt,
                system_prompt=system_prompt,
                history_messages=history_messages,
                image_data=image_data,
                **kwargs
            )
        
        # ä½¿ç”¨å·²å­˜åœ¨çš„ LightRAG å®ä¾‹åˆå§‹åŒ– RAGAnything
        self.rag = RAGAnything(
            lightrag=self.lightrag_instance,
            vision_model_func=vision_model_func,
        )
        
        print("âœ… RAG ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        
    async def build_database(self, file_path: str):
        """æ„å»ºæ•°æ®åº“"""
        if self.rag is None:
            await self.initialize_with_existing_lightrag()
            
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        supported_formats = ['.pdf', '.docx', '.txt', '.md', '.html', '.epub']
        file_extension = os.path.splitext(file_path)[1].lower()
        
        if file_extension not in supported_formats:
            supported_list = ', '.join(supported_formats)
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}ã€‚æ”¯æŒçš„æ ¼å¼: {supported_list}")
            
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(WORKING_DIR, exist_ok=True)
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        # Process Documents
        print(f"ğŸ“„ Process Documents: {file_path}")
        print(f"ğŸ“ æ–‡ä»¶ç±»å‹: {file_extension}")
        
        await self.rag.process_document_complete(
            file_path=file_path,
            output_dir=OUTPUT_DIR,
            parse_method="auto"
        )
        print("âœ… æ•°æ®åº“æ„å»ºå®Œæˆ")
        
        # ä¿å­˜æ•°æ®åº“çŠ¶æ€
        db_info = {
            "status": "built",
            "file_path": file_path,
            "file_type": file_extension,
            "build_time": asyncio.get_event_loop().time()
        }
        with open(DB_INFO_FILE, 'w', encoding='utf-8') as f:
            json.dump(db_info, f, ensure_ascii=False, indent=2)
        
    async def load_database(self):
        """åŠ è½½å·²å­˜åœ¨çš„æ•°æ®åº“"""
        if not os.path.exists(DB_INFO_FILE):
            raise Exception("æ•°æ®åº“ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ build_database.py æ„å»ºæ•°æ®åº“")
            
        if self.rag is None:
            await self.initialize_with_existing_lightrag()
            
        print("âœ… æ•°æ®åº“åŠ è½½å®Œæˆ")
        
    async def query(self, question: str, mode: str = "hybrid"):
        """æ–‡æœ¬æŸ¥è¯¢"""
        if self.rag is None:
            await self.load_database()
            
        try:
            return await self.rag.aquery(question, mode=mode)
        except Exception as e:
            print(f"âš ï¸ æŸ¥è¯¢å‡ºç°å¼‚å¸¸ï¼Œå°è¯•å¤‡ç”¨æ–¹æ³•: {e}")
            # å¤‡ç”¨æŸ¥è¯¢æ–¹æ³•
            if self.lightrag_instance:
                return await self.lightrag_instance.aquery(question, mode=mode)
            raise e
        
    async def multimodal_query(self, question: str, multimodal_content: List[Dict], mode: str = "hybrid"):
        """å¤šæ¨¡æ€æŸ¥è¯¢"""
        if self.rag is None:
            await self.load_database()
            
        return await self.rag.aquery_with_multimodal(
            question,
            multimodal_content=multimodal_content,
            mode=mode
        )