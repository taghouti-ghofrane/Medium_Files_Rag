import asyncio
import aiohttp
from typing import List, Dict, Any, Optional
from lightrag  import LightRAG
from lightrag.utils import EmbeddingFunc
# å‡è®¾ lightrag çš„æ£€ç´¢æ¨¡å—ç»“æ„ï¼Œå…·ä½“å¯¼å…¥è·¯å¾„å¯èƒ½éœ€è¦æ ¹æ®ä½ çš„ lightrag ç‰ˆæœ¬è°ƒæ•´
# å¸¸è§çš„å¯èƒ½æ˜¯ lightrag.operate.retrieval æˆ–ç±»ä¼¼
# å¦‚æœä¸‹é¢çš„å¯¼å…¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä½ çš„ lightrag å®‰è£…ç›®å½•ä¸‹çš„æºç ç»“æ„
try:
    from lightrag.operate import retrieval
    RETRIEVAL_AVAILABLE = True
except ImportError:
    RETRIEVAL_AVAILABLE = False
    print("Warning: Could not import lightrag.operate.retrieval. Retrieval details might not be available.")
import os
import logging
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
LLM_MODEL = "qwen3:8b"
EMBED_MODEL = "bge-m3:latest"
VLM_MODEL = "llava:latest"

# =======================
# å·¥å…·å‡½æ•°ï¼šè°ƒç”¨ Ollama APIï¼ˆå¼‚æ­¥ï¼‰
# ... (ä¿æŒä¸å˜) ...
# =======================

async def ollama_complete_async(
    model: str,
    prompt: str,
    system_prompt: Optional[str] = None,
    history_messages: List[Dict[str, str]] = [],
    image_data: Optional[str] = None,
    **kwargs
) -> str:
    """
    å¼‚æ­¥è°ƒç”¨ Ollamaï¼Œæ”¯æŒæ–‡æœ¬ + å›¾åƒè¾“å…¥
    """
    messages = []

    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})

    for msg in history_messages:
        messages.append({"role": msg["role"], "content": msg["content"]})

    # æ„é€  contentï¼šæ”¯æŒæ–‡æœ¬ + å›¾åƒ
    content = [{"type": "text", "text": prompt}]
    if image_data:
        content.append({
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}
        })

    messages.append({"role": "user", "content": content})

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
    }

    async with aiohttp.ClientSession() as session:
        async with session.post("http://localhost:11434/v1/chat/completions", json=payload) as resp:
            if resp.status != 200:
                text = await resp.text()
                raise Exception(f"Ollama error {resp.status}: {text}")
            result = await resp.json()
            return result["choices"][0]["message"]["content"]


async def ollama_embed_async(texts: List[str], model: str = "bge-m3:latest") -> List[List[float]]:
    """
    å¼‚æ­¥è·å–åµŒå…¥å‘é‡
    """
    embeddings = []
    async with aiohttp.ClientSession() as session:
        for text in texts:
            if not text.strip():
                embeddings.append([0.0] * 1024)
                continue
            payload = {"model": model, "input": text}
            try:
                async with session.post("http://localhost:11434/api/embeddings", json=payload) as resp:
                    if resp.status != 200:
                        text_resp = await resp.text()
                        logger.warning(f"Embedding error for text '{text[:50]}...': {text_resp}")
                        embeddings.append([0.0] * 1024)
                    else:
                        result = await resp.json()
                        embedding = result["embedding"]
                        # è¡¥é½æˆ–æˆªæ–­åˆ° 1024 ç»´åº¦
                        if len(embedding) > 1024:
                            embedding = embedding[:1024]
                        elif len(embedding) < 1024:
                            embedding += [0.0] * (1024 - len(embedding))
                        embeddings.append(embedding)
            except Exception as e:
                logger.warning(f"Embedding exception for text '{text[:50]}...': {e}")
                embeddings.append([0.0] * 1024)
    return embeddings


# =======================
# åŒ…è£…å‡½æ•°ï¼ˆåœ¨å¼‚æ­¥ç¯å¢ƒä¸­ç›´æ¥è°ƒç”¨ï¼‰
# =======================

async def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
    return await ollama_complete_async(
        model=LLM_MODEL,
        prompt=prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        **kwargs
    )

# async def embedding_func(texts):
#     return await ollama_embed_async(texts, EMBED_MODEL)
from build_database import RAGDatabaseManager

async def embedding_func(texts):
    db_manager = RAGDatabaseManager(embed_model="text-embedding-v4")
    await db_manager._create_rag_instance()
    embed_func = db_manager.embedding_func.func  # æå–çœŸæ­£çš„å¼‚æ­¥å‡½æ•°

    embeddings = await embed_func(texts)
    return embeddings

async def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs):
    return await ollama_complete_async(
        model=VLM_MODEL,
        prompt=prompt,
        system_prompt=system_prompt,
        history_messages=history_messages,
        image_data=image_data,
        **kwargs
    )

# =======================
# LightRAG é…ç½®
# =======================

lightrag_working_dir = "./rag_storage_new"

# åˆ›å»ºæˆ–åŠ è½½ LightRAG å®ä¾‹
lightrag_instance = LightRAG(
    working_dir=lightrag_working_dir,
    llm_model_func=llm_model_func,
    embedding_func=EmbeddingFunc(
        embedding_dim=1024,
        max_token_size=512,
        func=embedding_func
    )
)

# =======================
# ç¤ºä¾‹æŸ¥è¯¢å¹¶å°è¯•è¾“å‡ºå‘½ä¸­çš„çŸ¥è¯†åº“æ¡ç›®
# =======================

async def main():
    print("âœ… æ­£åœ¨åŠ è½½å·²å­˜åœ¨çš„ LightRAG å®ä¾‹...")
    await lightrag_instance.initialize_storages()

    query_text = "è¯­ä¹‰å›¾åƒå¦‚ä½•æ„å»º"

    # 1. å°è¯•é€šè¿‡åº•å±‚æ£€ç´¢æ¨¡å—è·å– chunks
    print(f"\nğŸ” æ­£åœ¨æ£€ç´¢ä¸ '{query_text}' ç›¸å…³çš„çŸ¥è¯†åº“æ¡ç›®...")
    
    if RETRIEVAL_AVAILABLE and hasattr(lightrag_instance, 'chunk_vdb') and hasattr(lightrag_instance, 'embedding_func'):
        try:
            # ä½¿ç”¨ lightrag å†…éƒ¨çš„ retrieval æ¨¡å—è¿›è¡Œæ£€ç´¢
            # è¿™é€šå¸¸éœ€è¦ query_text, top_k, chunk_db, embed_func ç­‰å‚æ•°
            # æ³¨æ„ï¼šè¿™ä¸ªè°ƒç”¨çš„å…·ä½“ç­¾åä¾èµ–äº lightrag çš„ç‰ˆæœ¬
            # å‡è®¾æœ‰ä¸€ä¸ª text_retrieve å‡½æ•°
            if hasattr(retrieval, 'text_retrieve'):
                 # å‡è®¾ text_retrieve çš„ç­¾åç±»ä¼¼äº:
                 # async def text_retrieve(query, EmbeddingFunc, chunk_db, top_k: int = 10)
                 retrieved_chunks = await retrieval.text_retrieve(
                     query=query_text,
                     embedding_func=lightrag_instance.embedding_func, # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½éœ€è¦ç›´æ¥ä¼ å…¥ func
                     chunk_db=lightrag_instance.chunk_vdb,
                     top_k=5
                 )
                 print(f"\nğŸ“„ é€šè¿‡ retrieval æ¨¡å—æ£€ç´¢åˆ°çš„ Top {len(retrieved_chunks)} çŸ¥è¯†åº“æ¡ç›®:")
                 if retrieved_chunks:
                     # retrieved_chunks çš„æ ¼å¼å¯èƒ½æ˜¯ä¸€ä¸ªåŒ…å« id å’Œ/æˆ– content çš„åˆ—è¡¨æˆ–å­—å…¸
                     # éœ€è¦æ ¹æ®å®é™…è¿”å›æ ¼å¼è°ƒæ•´
                     for i, chunk_info in enumerate(retrieved_chunks):
                         print(f"\n--- æ¡ç›® {i+1} ---")
                         # å°è¯•æ‰“å°å¯ç”¨ä¿¡æ¯
                         if isinstance(chunk_info, dict):
                             for key, value in chunk_info.items():
                                 if key != 'content' or len(str(value)) < 200: # é¿å…æ‰“å°è¿‡é•¿å†…å®¹
                                     print(f"{key}: {value}")
                                 else:
                                     print(f"{key}: {str(value)[:200]}...")
                         else:
                             print(f"Chunk Info: {chunk_info}")
                 else:
                     print("æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³æ¡ç›®ã€‚")
            else:
                print("lightrag.operate.retrieval æ¨¡å—ä¸­æœªæ‰¾åˆ° text_retrieve å‡½æ•°ã€‚")
        except Exception as e:
            print(f"é€šè¿‡ retrieval æ¨¡å—æ£€ç´¢æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("æ— æ³•ä½¿ç”¨åº•å±‚ retrieval æ¨¡å—è¿›è¡Œæ£€ç´¢ (æ¨¡å—æœªå¯¼å…¥æˆ– LightRAG å®ä¾‹ç¼ºå°‘å¿…è¦å±æ€§)ã€‚")

    # 2. å¦‚æœä¸Šè¿°æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ç›´æ¥è®¿é—® chunk_vdb çš„ aquery æ–¹æ³• (å¦‚æœå­˜åœ¨)
    if hasattr(lightrag_instance, 'chunk_vdb'):
        try:
            query_embedding = await embedding_func([query_text])
            # å°è¯•ç›´æ¥æŸ¥è¯¢ chunk_vdb
            # å‡è®¾ aquery æ–¹æ³•æ¥å— embedding å‘é‡å’Œ top_k å‚æ•°
            # æ³¨æ„ï¼šAPI å¯èƒ½æ˜¯ query, aquery, search ç­‰ï¼Œå‚æ•°ä¹Ÿå¯èƒ½ä¸åŒ
            if hasattr(lightrag_instance.chunk_vdb, 'query') or hasattr(lightrag_instance.chunk_vdb, 'aquery') or hasattr(lightrag_instance.chunk_vdb, 'search'):
                # å°è¯•æœ€å¸¸è§çš„ aquery
                vdb_query_func = getattr(lightrag_instance.chunk_vdb, 'aquery', getattr(lightrag_instance.chunk_vdb, 'query', getattr(lightrag_instance.chunk_vdb, 'search', None)))
                if vdb_query_func:
                    # æ³¨æ„ï¼šè¿™é‡Œçš„å‚æ•°éœ€è¦æ ¹æ® nano-vectordb çš„å®é™… API è°ƒæ•´
                    # å¸¸è§çš„å¯èƒ½æ˜¯ aquery(query_vector, top_k)
                    top_k = 5
                    # query_embedding[0] æ˜¯ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€ä¸€ä¸ªï¼‰æŸ¥è¯¢æ–‡æœ¬çš„ embedding å‘é‡
                    searched_result = await vdb_query_func(query_embedding[0], top_k=top_k)
                    
                    print(f"\nğŸ“„ é€šè¿‡ç›´æ¥æŸ¥è¯¢ chunk_vdb æ£€ç´¢åˆ°çš„ Top {top_k} çŸ¥è¯†åº“æ¡ç›®:")
                    # nano-vectordb çš„ aquery é€šå¸¸è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« 'distances', 'ids', 'metadatas' (å¦‚æœå­˜å‚¨äº†), 'documents' (å¦‚æœå­˜å‚¨äº†åŸå§‹æ–‡æœ¬)
                    # æˆ‘ä»¬ä¸»è¦å…³å¿ƒ 'metadatas' æˆ– 'documents'
                    if isinstance(searched_result, dict):
                        # å‡è®¾ 'metadatas' åŒ…å«äº†æˆ‘ä»¬å­˜å‚¨çš„ chunk ä¿¡æ¯ (content, file_path ç­‰)
                        metadatas = searched_result.get('metadatas', [])
                        documents = searched_result.get('documents', []) # å¦‚æœå­˜å‚¨äº†åŸå§‹æ–‡æ¡£
                        ids = searched_result.get('ids', [])
                        
                        for i, (chunk_id, metadata) in enumerate(zip(ids, metadatas)):
                             print(f"\n--- æ¡ç›® {i+1} ---")
                             print(f"ID: {chunk_id}")
                             if isinstance(metadata, dict):
                                 # æ‰“å°å…ƒæ•°æ®ä¸­çš„å…³é”®ä¿¡æ¯
                                 content = metadata.get('content', 'N/A')
                                 file_path = metadata.get('file_path', 'N/A')
                                 full_doc_id = metadata.get('full_doc_id', 'N/A')
                                 print(f"å†…å®¹é¢„è§ˆ: {content[:200]}..." if content and len(content) > 200 else f"å†…å®¹: {content}")
                                 print(f"æ¥æºæ–‡ä»¶: {file_path}")
                                 print(f"æ–‡æ¡£ ID: {full_doc_id}")
                             else:
                                 print(f"Metadata: {metadata}")
                             
                             # å¦‚æœ documents åˆ—è¡¨ä¹Ÿæœ‰å¯¹åº”å†…å®¹ï¼Œä¹Ÿå¯ä»¥æ‰“å°
                             if i < len(documents):
                                 doc_content = documents[i]
                                 if doc_content and doc_content != metadata.get('content'): # é¿å…é‡å¤æ‰“å°
                                     print(f"æ–‡æ¡£å†…å®¹ (æ¥è‡ª documents): {doc_content[:100]}...")
                    else:
                        print(f"æ£€ç´¢ç»“æœæ ¼å¼æœªçŸ¥: {type(searched_result)}")
                else:
                    print("chunk_vdb æ²¡æœ‰æ‰¾åˆ° query/aquery/search æ–¹æ³•ã€‚")
            else:
                print("chunk_vdb æ²¡æœ‰æ‰¾åˆ° query/aquery/search æ–¹æ³•ã€‚")
                
        except Exception as e:
             print(f"ç›´æ¥æŸ¥è¯¢ chunk_vdb æ—¶å‘ç”Ÿé”™è¯¯: {e}")
             import traceback
             traceback.print_exc()
    else:
         print("LightRAG å®ä¾‹æ²¡æœ‰ chunk_vdb å±æ€§ã€‚")


    # 3. æ‰§è¡ŒæŸ¥è¯¢å¹¶ç”Ÿæˆç­”æ¡ˆ (å¯é€‰)
    print(f"\nğŸ¤– æ­£åœ¨ç”Ÿæˆé’ˆå¯¹ '{query_text}' çš„å›ç­”...")
    try:
        result = await lightrag_instance.aquery(query_text)
        print(f"\nğŸ’¬ æŸ¥è¯¢ç»“æœ: {result}")
    except Exception as e:
        print(f"æŸ¥è¯¢ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())